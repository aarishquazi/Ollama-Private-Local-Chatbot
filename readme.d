# ğŸ§  Local Document Chatbot

A private, offline chatbot that can:
- Chat with you conversationally
- Answer questions based on your uploaded documents (RAG-style)
- Use open-source LLMs via [Ollama](https://ollama.com)
- Run entirely on your local machine â€” no API keys or internet required

---

## ğŸš€ Features

- ğŸ’¬ **Conversational Mode** â€“ Natural, context-aware chat
- ğŸ“„ **Document Question Answering** â€“ Asks and answers questions from uploaded PDFs or files
- ğŸ” **Offline & Private** â€“ Powered by local models using Ollama, no data sent outside
- âš¡ **Streamlit UI** â€“ Simple and fast web interface
- ğŸ§  **Intent Detection** â€“ Smart routing of questions vs. chat using LLM-based classification

---

## ğŸ› ï¸ Tech Stack

- [Streamlit](https://streamlit.io) â€“ frontend interface
- [Ollama](https://ollama.com) â€“ local LLM runtime (e.g., `llama3`, `mistral`)
- [ChromaDB](https://www.trychroma.com) â€“ local vector store
- [Python](https://www.python.org/) â€“ backend logic
- Optional: `LangChain`, `PyPDF`, `FAISS`, or others if customized

---

## ğŸ“¦ Installation

1. **Clone the repo**
   ```bash
   git clone https://github.com/your-username/local-document-chatbot.git
   cd local-document-chatbot

2. **Create a virtual environment**

```bash
    python -m venv env
    source env/bin/activat
    
3. **Install dependencies**

    ```bash
    pip install -r requirements.txt

4. **Install and run Ollama**

    - Download Ollama

    - tart a model (e.g., llama3)

    ```bash
    ollama run llama3
    Run the app
    streamlit run main.py

## ğŸ“Œ Notes

- Make sure Ollama is running before starting the app.

- Document-based Q&A is only triggered when your input is a question (classified by the model).
